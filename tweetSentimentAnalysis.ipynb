{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "tweetSentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A_ZSHGgv9RB",
        "colab_type": "text"
      },
      "source": [
        "## Tweet analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88xS7ecnv9RE",
        "colab_type": "text"
      },
      "source": [
        "# Google Collab With TPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6o9igBw2j_V_",
        "colab": {}
      },
      "source": [
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "# !tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "# !pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vx869UikkAfQ",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oQL9iLEnoUMZ",
        "colab": {}
      },
      "source": [
        "# import findspark\n",
        "# findspark.init()\n",
        "# from pyspark.sql import SparkSession\n",
        "# spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dnb8EfTPSrv8",
        "outputId": "a4400fdd-7f6a-427a-9552-8bc2b63c7f05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ofH4ilwv9RS",
        "colab_type": "text"
      },
      "source": [
        "# importing all libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Te70vsuNoXHG",
        "outputId": "eaec28d7-dc57-494b-bc76-db67df4e0adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sys\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "from sys import path\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "from string import punctuation, digits\n",
        "from IPython.core.display import display, HTML\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# from pyspark.sql import Row\n",
        "# from pyspark.sql import SQLContext\n",
        "# from pyspark import SparkFiles\n",
        "# from pyspark.ml.linalg import DenseVector\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# import pyspark\n",
        "# from pyspark import SparkContext\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from io import StringIO\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.metrics import classification_report,recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from sklearn.metrics import roc_curve,precision_recall_curve\n",
        "from sklearn.metrics import roc_auc_score,confusion_matrix\n",
        "\n",
        "# from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "# from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "# from pyspark.ml.feature import StringIndexer\n",
        "# from pyspark.ml import Pipeline\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYNkYegsv9RW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EHb04hx_pG2l",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/twitter_training.csv\",encoding='latin',header=None)\n",
        "df=df[[0,5]]\n",
        "df=df.rename(columns={0: \"Class\", 5: \"Tweet\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOh_CSInxPjT",
        "colab_type": "text"
      },
      "source": [
        "Converting Postive sentiments which were denoted by 4 with 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bk9qpS5B4y-j",
        "colab": {}
      },
      "source": [
        "df.Class=df.Class.replace(4,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2KHHCeCH2rt6",
        "outputId": "fa2acd85-76cf-4514-b640-7340a8324454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1600000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D2a6KFe35HIm",
        "outputId": "b89f19ce-2838-471f-c296-a85472a54b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>1</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>1</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>1</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>1</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>1</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Class                                              Tweet\n",
              "0            0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1            0  is upset that he can't update his Facebook by ...\n",
              "2            0  @Kenichan I dived many times for the ball. Man...\n",
              "3            0    my whole body feels itchy and like its on fire \n",
              "4            0  @nationwideclass no, it's not behaving at all....\n",
              "...        ...                                                ...\n",
              "1599995      1  Just woke up. Having no school is the best fee...\n",
              "1599996      1  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599997      1  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599998      1  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599999      1  happy #charitytuesday @theNSPCC @SparksCharity...\n",
              "\n",
              "[1600000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpzX1OsSv9Rg",
        "colab_type": "text"
      },
      "source": [
        "# converting all the text data to lower letter and remove links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RUWoZmNiR8F0",
        "colab": {}
      },
      "source": [
        "df['Lower']=df['Tweet'].str.lower()\n",
        "df['Lower']=df['Lower'].str.replace('\\d+', '')\n",
        "df['Lower']=df['Lower'].str.replace(r\"http\\S+\", '')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kagtIjbrv9Ri",
        "colab_type": "text"
      },
      "source": [
        "# removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6MYhNO69RcDV",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "df['tweet_without_stopwords'] = df['Lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj21LSXEv9Rk",
        "colab_type": "text"
      },
      "source": [
        "# Stemming and Lemmatisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S1iKujGmRjHA",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Use English stemmer.\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "lem=WordNetLemmatizer()\n",
        "\n",
        "df['lemm'] = df['tweet_without_stopwords'].apply(lambda x: ' '.join([lem.lemmatize(y) for y in x.split()]))\n",
        "df['stemmed'] = df['lemm'].apply(lambda x: ' '.join([stemmer.stem(y) for y in x.split()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1quyZ1vmv9Rl",
        "colab_type": "text"
      },
      "source": [
        "# Keeping only english letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CncDmY8ARsK5",
        "colab": {}
      },
      "source": [
        "df['cleaned_specials'] = df['stemmed'].str.replace('[^A-z]', '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4lmOC9lASnW3",
        "outputId": "b5270b8e-affb-40f0-ed56-28a7be1f55b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df2=df\n",
        "df.columns"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Class', 'Tweet', 'Lower', 'tweet_without_stopwords', 'lemm', 'stemmed',\n",
              "       'cleaned_specials'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te-WZY1av9Rn",
        "colab_type": "text"
      },
      "source": [
        "# Dropping all the intermediate column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bk_Ab2ltTXH9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "00434fb1-5e6a-4215-81ca-beaf031e970e"
      },
      "source": [
        "df=df.drop(columns=['Lower','tweet_without_stopwords','lemm','stemmed','Tweet'])\n",
        "df=df.rename(columns={'cleaned_specials': \"Tweet\"})\n",
        "df\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>switchfoot  awww that bummer shoulda got david...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>upset cant updat facebook text it might cri re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>kenichan dive mani time ball manag save  rest ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>whole bodi feel itchi like fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>nationwideclass no behav all im mad here cant ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>1</td>\n",
              "      <td>woke up school best feel ever</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>1</td>\n",
              "      <td>thewdbcom  cool hear old walt interviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>1</td>\n",
              "      <td>readi mojo makeover ask detail</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>1</td>\n",
              "      <td>happi th birthday boo alll time tupac amaru sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>1</td>\n",
              "      <td>happi charitytuesday thenspcc sparkschar speak...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Class                                              Tweet\n",
              "0            0  switchfoot  awww that bummer shoulda got david...\n",
              "1            0  upset cant updat facebook text it might cri re...\n",
              "2            0  kenichan dive mani time ball manag save  rest ...\n",
              "3            0                    whole bodi feel itchi like fire\n",
              "4            0  nationwideclass no behav all im mad here cant ...\n",
              "...        ...                                                ...\n",
              "1599995      1                      woke up school best feel ever\n",
              "1599996      1          thewdbcom  cool hear old walt interviews \n",
              "1599997      1                     readi mojo makeover ask detail\n",
              "1599998      1  happi th birthday boo alll time tupac amaru sh...\n",
              "1599999      1  happi charitytuesday thenspcc sparkschar speak...\n",
              "\n",
              "[1600000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q-JvITqyNkJ",
        "colab_type": "text"
      },
      "source": [
        "# Creating TFID Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K0BmsyxVxCtS",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zD008kGl3DLq",
        "outputId": "4cf73cd6-c5da-445a-c761-a018a2b24f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "vectorizer.fit(df['Tweet'])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yaktdYZF3GdG",
        "colab": {}
      },
      "source": [
        "vector = vectorizer.transform(df['Tweet'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QtvK71wv9Rs",
        "colab_type": "text"
      },
      "source": [
        "# Pickle the vectorizer for model to be used in the application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PXdja0AVY77-",
        "outputId": "9a2f63e4-5c16-4d09-9ad6-78e023e85d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.externals import joblib \n",
        "# Save the model as a pickle in a file \n",
        "joblib.dump(vectorizer, 'abc.pkl')\n",
        "# vector"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abc.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dh081wjv9Rt",
        "colab_type": "text"
      },
      "source": [
        "# Splitting in Train and Test in 70:30 ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YSnOIvLu3PzD",
        "outputId": "9e040f1b-3851-411b-9c82-242f6c5c8eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# splitting dataset into training and test data\n",
        "trainX, testX = train_test_split(vector, test_size=0.30, random_state=1)\n",
        "trainY, testY = train_test_split(df['Class'], test_size=0.30, random_state=1)\n",
        "trainX\n",
        "\n",
        "xTrain, xTest = train_test_split(vector, test_size=0.30, random_state=1)\n",
        "yTrain, yTest = train_test_split(df['Class'], test_size=0.30, random_state=1)\n",
        "trainX\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1120000x714322 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 7301901 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frpyjU5nv9Ru",
        "colab_type": "text"
      },
      "source": [
        "# Applying Multinomial NB Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k3dki6Ps3gbv",
        "outputId": "33ebd66b-dc35-4d94-a9ae-0bd84874c096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Fitting Naibe Bayes to the training set\n",
        "classifier = MultinomialNB().fit(trainX ,trainY)\n",
        "from sklearn.externals import joblib \n",
        "# Save the model as a pickle in a file \n",
        "joblib.dump(MultinomialNB, 'nb_Down.pkl')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nb_Down.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-oKWonf13mCB",
        "colab": {}
      },
      "source": [
        "all_predictions = classifier.predict(testX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "je62TJHW3oox",
        "outputId": "78ed10f2-e3d6-49c4-a799-9a421670950b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Calculating and printing the model accuracy and f1 score\n",
        "print (classification_report(testY, all_predictions))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.78      0.76    239996\n",
            "           1       0.77      0.73      0.75    240004\n",
            "\n",
            "    accuracy                           0.75    480000\n",
            "   macro avg       0.75      0.75      0.75    480000\n",
            "weighted avg       0.75      0.75      0.75    480000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVvbGU4Zv9Rx",
        "colab_type": "text"
      },
      "source": [
        "# Applying Logistic Regression Classifier and pickling it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fqnoVLuv3qpd",
        "outputId": "28290e14-41c5-44ea-febc-61e4cfe6b1d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Fitting LR to the training set\n",
        "classifier2 = LogisticRegression(random_state=1)\n",
        "classifier2.fit(trainX, trainY)\n",
        "\n",
        "#Predicting the test set results\n",
        "y_pred = classifier2.predict(testX)\n",
        "\n",
        "# Calculating and printing the model accuracy and f1 score\n",
        "print (classification_report(testY, y_pred))\n",
        "\n",
        "\n",
        "from sklearn.externals import joblib \n",
        "# Save the model as a pickle in a file \n",
        "joblib.dump(classifier2, 'lr_Down.pkl')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.74      0.76    239996\n",
            "           1       0.76      0.80      0.78    240004\n",
            "\n",
            "    accuracy                           0.77    480000\n",
            "   macro avg       0.77      0.77      0.77    480000\n",
            "weighted avg       0.77      0.77      0.77    480000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lr_Down.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FM5RLLpv9Rz",
        "colab_type": "text"
      },
      "source": [
        "# Applying SGDC Classifier and pickling it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kgLJfaWT3txY",
        "outputId": "41688a45-1f89-463e-f9db-c3f17d04fb0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Fitting Stochastic Gradient Descent to the training set\n",
        "classifier3 = SGDClassifier(random_state=1)\n",
        "classifier3.fit(trainX, trainY)\n",
        "\n",
        "#Predicting the test set results\n",
        "y_prediction = classifier3.predict(testX)\n",
        "\n",
        "# Calculating and printing the model accuracy and f1 score\n",
        "print (classification_report(testY, y_prediction))\n",
        "\n",
        "from sklearn.externals import joblib \n",
        "# Save the model as a pickle in a file \n",
        "joblib.dump(classifier3, 'sgdc_Down.pkl')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.64      0.71    239996\n",
            "           1       0.70      0.84      0.77    240004\n",
            "\n",
            "    accuracy                           0.74    480000\n",
            "   macro avg       0.75      0.74      0.74    480000\n",
            "weighted avg       0.75      0.74      0.74    480000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sgdc_Down.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NExewcyXv9R0",
        "colab_type": "text"
      },
      "source": [
        "# MaxAbs Scalar to scale the data ,as normal data was huge for SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JcNknUbjZHJI",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "scaling=MaxAbsScaler().fit(xTrain)\n",
        "xTrain=scaling.transform(xTrain)\n",
        "xTest=scaling.transform(xTest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7D3jROlv9R1",
        "colab_type": "text"
      },
      "source": [
        "# Applying Linear SVC Model and pickling it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "txNiBIb536LQ",
        "outputId": "41dadcf6-d238-450b-b62d-7a8a5e248ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "classifier3 = LinearSVC()\n",
        "classifier3.fit(xTrain, yTrain)\n",
        "\n",
        "from sklearn.externals import joblib \n",
        "# Save the model as a pickle in a file \n",
        "joblib.dump(classifier3, 'linearsvc_Down.pkl')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['linearsvc_Down.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vlgm3L3U4eVe",
        "outputId": "4ebf96af-20b9-4c6d-a405-a9eea0c08255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "y_prediction = classifier3.predict(xTest)\n",
        "# Calculating and printing the model accuracy and f1 score\n",
        "print (classification_report(testY, y_prediction))\n",
        "# SVC()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.74      0.75    239996\n",
            "           1       0.75      0.77      0.76    240004\n",
            "\n",
            "    accuracy                           0.76    480000\n",
            "   macro avg       0.76      0.76      0.76    480000\n",
            "weighted avg       0.76      0.76      0.76    480000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI1e2k3jv9R2",
        "colab_type": "text"
      },
      "source": [
        "# Applying Random Forest Classifer and pickling it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X2CdGfZToc1F",
        "outputId": "261c1a16-6cdc-46cd-cb9c-037789eabc72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_classifier = RandomForestClassifier(n_jobs=-1,max_depth=3,random_state=1,warm_start=True)\n",
        "rf_classifier.fit(xTrain,yTrain)\n",
        "from sklearn.externals import joblib \n",
        "# Save the model as a pickle in a file \n",
        "joblib.dump(rf_classifier, 'rf_Down.pkl')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rf_Down.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hLRLDx6codsP",
        "outputId": "5849a6bb-6392-45f9-cdcd-215ddbbe9f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model2 = rf_classifier.predict(xTest)\n",
        "#print(classification_report(yTest, model2))\n",
        "print('The accuracy :- ',accuracy_score(yTest, model2)*100)\n",
        "print (classification_report(testY, model2))\n",
        "\n",
        "rfAccuracy=accuracy_score(yTest, model2)*100"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy :-  51.90833333333333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.06      0.12    239996\n",
            "           1       0.51      0.97      0.67    240004\n",
            "\n",
            "    accuracy                           0.52    480000\n",
            "   macro avg       0.61      0.52      0.39    480000\n",
            "weighted avg       0.61      0.52      0.39    480000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V8WVwFxn4nS1",
        "colab": {}
      },
      "source": [
        "# model=tree.DecisionTreeClassifier(criterion=\"entropy\",max_depth=3,random_state=1)\n",
        "# model.fit(xTrain,yTrain)\n",
        "# from sklearn.externals import joblib \n",
        "# # Save the model as a pickle in a file \n",
        "# joblib.dump(model, 'rf_Down.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovgUgfT77ed6",
        "colab": {}
      },
      "source": [
        "# model2 = model.predict(xTest)\n",
        "# print(classification_report(yTest, model2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sv51WYNb6h_w",
        "colab": {}
      },
      "source": [
        "# jhvakcvlb a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kc-veEm_-jnH",
        "colab": {}
      },
      "source": [
        "# neigh = KNeighborsClassifier(n_neighbors=2,n_jobs=-1,leaf_size=400)\n",
        "# neigh.fit(xTrain, yTrain)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j85i5x2DBqSD",
        "colab": {}
      },
      "source": [
        "# neigh2 = neigh.predict(xTest)\n",
        "# # #print(classification_report(yTest, neigh2))\n",
        "# print('The accuracy :- ',accuracy_score(yTest, neigh2)*100)\n",
        "\n",
        "# knnAccuracy=accuracy_score(yTest, model2)*100\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}